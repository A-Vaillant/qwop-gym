---
# [int] (optional) Env seed (auto-generated if blank)
seed: ~

# [string] (optional) Unique ID of this run (auto-generated if blank)
run_id: ~

# [string] (optional) Continue training of a previously trained model
model_load_file: ~

# [string] Directory template to save the trained model, metadata and
# tensorboard logs. If the template contains {run_id} or {seed} placeholders,
# they will be replaced with the corresponding runtime values.
out_dir_template: "data/PPO-{run_id}"

# [bool] Log tensorboard data
log_tensorboard: true

# [int] Training duration
total_timesteps: 100_000

# [int] (optional) Force env termination on the Nth step of an episode
max_episode_steps: 5000

# [int] Number of times the model will be saved during training
# Example: 5 means save at 20%, 40%, 60%, 80% and 100% progress
n_checkpoints: 5

# PPO algorithm parameters (used only if `model_load_file` is blank)
# https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO
learner_kwargs:
  policy: "MlpPolicy"
  use_sde: false
  sde_sample_freq: 4

  # NOTE: n_envs=1 => the rollout buffer size is 64
  # However, n_epochs=10 means each train() cycle uses 10 * 32 buffer samples
  # Buffer is then wiped and 64 new experiences are collected.
  # isn't that super overfitting? Maybe increase n_steps to 2048 (the default)
  n_steps: 64
  batch_size: 32
  n_epochs: 10

  gamma: 0.9
  gae_lambda: 0.98
  clip_range: 0.4
  normalize_advantage: true
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 3

# Schedule for the learning_rate as a function of the progress remaining.
# Possible values for `fn`:
#   * "const" - use `initial_value` as a constant value for the entire training
#   * "lin_decay" - starting with `initial_value`, multiply by `step` at even
#      intervals (but no more than `decays` times) until `min_value` is reached
# Example:
#   fn: "lin_decay"
#   initial_value: 0.01
#   step: 0.5
#   decays: 10
#   min_value: 0.001
# During training, the learning_rate (lr) changes like so:
# 0%: 0.01, 10%: 0.005, 20%: 0.0025, 30%: 0.000625, 40%: 0.000625, ...100%: 0.000625
# (0.000625 is below `min_value` so the learning rate remains unchanged)
learner_lr_schedule:
  fn: "const"
  initial_value: 0.003
  step: 0.75
  decays: 20
  min_value: 5.0e-05

# Env parameters
# The special "__include__" key allows to load them from another file.
# Keys listed here take precedence over keys loaded with __include__.
# See notes in `env.yml` for more info
env_kwargs:
  __include__: "config/env.yml"
  text_in_browser: "Do not close this window, training in progress..."

# List of gym wrappers to use for the env
# Each list element must be a dict with "module", "cls" and "kwargs" keys
env_wrappers: []
