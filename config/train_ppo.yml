---
seed: ~  # auto-generated if blank
run_id: ~  # auto-generated if blank
out_dir_template: "data/PPO-{run_id}"
log_tensorboard: true
total_timesteps: !!float 10000
max_episode_steps: 5000
n_checkpoints: 5
learner_kwargs:
  policy: "MlpPolicy"
  use_sde: false
  sde_sample_freq: 4

  # NOTE: n_envs=1 => the rollout buffer size is 64
  # However, n_epochs=10 means each train() cycle uses 10 * 32 buffer samples
  # Buffer is then wiped and 64 new experiences are collected.
  # isn't that super overfitting? Maybe increase n_steps to 2048 (the default)
  n_steps: 64
  batch_size: 32
  n_epochs: 10

  gamma: 0.9
  gae_lambda: 0.98
  clip_range: 0.4
  normalize_advantage: true
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 3
learner_lr_schedule:
  fn: "const"
  initial_value: 0.003
  step: 0.75
  decays: 20
  min_value: 5.0e-05
env_kwargs:
  __include__: "config/env.yml"
  text_in_browser: "Do not close this window, training in progress..."
